#!/usr/bin/env python3
"""
reg-api-stats: Create statistics and perform checks on ingested datasets

Usage:
    ./reg-api-stats.py start
    ./reg-api-stats.py stop
    ./reg-api-stats.py status
"""

import os
import sys
import time
import signal
import json
from datetime import datetime, timedelta
import hashlib
import zipfile
import stat
import argparse

#Set current folder to the current script path
os.chdir(os.path.dirname(os.path.abspath(__file__)))

#Load configuration file
##We avoid to use pyyaml here, as we have a very simple YAML file
def read_simple_yaml(path):
    """
    Read a minimal YAML file with:
        group:
          key: value
    Returns a dict of dicts.
    """
    data = {}
    current_group = None

    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()

            # Skip blank lines and comments
            if not line or line.startswith('#'):
                continue

            # Group header
            if line.endswith(':') and not line.startswith(' '):
                current_group = line[:-1].strip()
                data[current_group] = {}
                continue

            # Key-value line inside a group
            if current_group and ':' in line:
                key, value = line.split(':', 1)
                data[current_group][key.strip()] = value.strip()
                continue

            # If badly formatted, you could raise an error
            # raise ValueError(f"Invalid YAML line: {line}")

    return data

def load_yaml(cfgfile):
  cfg=read_simple_yaml(cfgfile)
  #with open(cfgfile,'r') as f:
  #  cfg=yaml.safe_load(f)
  return cfg

def load_conf():
  CURPATH = os.path.dirname(os.path.realpath(__file__))
  CFGPATH = os.path.realpath(os.path.join(CURPATH,"../cfg"))
  CFGFILE = os.path.join(CFGPATH,"conf.yaml")
  cfg=load_yaml(CFGFILE)['stats']
  cfg['config_path']=CFGPATH
  return cfg

conf = load_conf()
PID_FILE=conf['pid']
LOG_FILE=conf['logfile']
CRON_STR=conf['cron']
CFGPATH=conf['config_path']
CFGFILE = os.path.join(CFGPATH,"conf.yaml")
del conf

def log(message: str):
    """Append a timestamped message to the log file."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a") as f:
        f.write(f"[{timestamp}] {message}\n")

def daemonize():
    """Detach process from terminal and run in background."""
    if os.fork() > 0:
        sys.exit(0)  # Exit parent

    log("Starting daemon...")
    os.setsid()

    if os.fork() > 0:
        sys.exit(0)  # Exit first child

    sys.stdout.flush()
    sys.stderr.flush()

    # Redirect standard file descriptors
    with open("/dev/null", "rb", 0) as f:
        os.dup2(f.fileno(), sys.stdin.fileno())
    with open("/dev/null", "ab", 0) as f:
        os.dup2(f.fileno(), sys.stdout.fileno())
        os.dup2(f.fileno(), sys.stderr.fileno())

    # Write PID file
    with open(PID_FILE, "w") as f:
        f.write(str(os.getpid()))

    log("Daemon started.")
    run_daemon()

def parse_cron_field(field: str, min_value: int, max_value: int):
    """Parse a single cron field into a set of valid integer values."""
    values = set()
    for part in field.split(','):
        if part == '*':
            values.update(range(min_value, max_value + 1))
        elif part.startswith('*/'):
            step = int(part[2:])
            values.update(range(min_value, max_value + 1, step))
        elif '-' in part:
            start, end = map(int, part.split('-'))
            values.update(range(start, end + 1))
        else:
            values.add(int(part))
    return values

def next_cron_datetime(cron_expr: str, start_time = None) -> datetime:
    """
    Compute the next datetime that matches the given 5-field cron expression.
    
    Supports: minute, hour, day of month, month, day of week.
    """
    if start_time is None:
        start_time = datetime.utcnow().replace(second=0, microsecond=0)
    else:
        start_time = start_time.replace(second=0, microsecond=0)

    minute_field, hour_field, dom_field, month_field, dow_field = cron_expr.split()

    minutes = parse_cron_field(minute_field, 0, 59)
    hours = parse_cron_field(hour_field, 0, 23)
    days = parse_cron_field(dom_field, 1, 31)
    months = parse_cron_field(month_field, 1, 12)
    dows = parse_cron_field(dow_field, 0, 6)  # 0=Sunday

    candidate = start_time + timedelta(minutes=1)

    while True:
        if (candidate.minute in minutes and
            candidate.hour in hours and
            candidate.day in days and
            candidate.month in months and
            candidate.weekday() in dows):
            return candidate

        candidate += timedelta(minutes=1)

def get_next_run_time():
    """Return datetime of next scheduled run."""

    now = datetime.now()
    next_run = next_cron_datetime(CRON_STR, now)

    return next_run

def run_daemon():
    """Main loop for daemon."""
    signal.signal(signal.SIGTERM, handle_exit)
    
    while True:
        next_run = get_next_run_time()
        next_run_interval_s = round((next_run - datetime.now()).total_seconds())
        log(f"Next run scheduled at {next_run}. In {next_run_interval_s} seconds.")
        time.sleep(next_run_interval_s)
        run_job()

def handle_exit(signum, frame):
    """Handle SIGTERM for clean shutdown."""
    log("Daemon stopping...")
    if os.path.exists(PID_FILE):
        os.remove(PID_FILE)
    log("Daemon stopped.")
    sys.exit(0)


def start():
    if os.path.exists(PID_FILE):
        print("Daemon already running.")
        sys.exit(1)

    daemonize()

def stop():
    if not os.path.exists(PID_FILE):
        print("Daemon not running.")
        return

    with open(PID_FILE, "r") as f:
        pid = int(f.read().strip())

    try:
        os.kill(pid, signal.SIGTERM)
        print("Daemon stopped.")
    except ProcessLookupError:
        print("Process not found. Removing stale PID file.")
    finally:
        if os.path.exists(PID_FILE):
            os.remove(PID_FILE)

def status():
    if os.path.exists(PID_FILE):
        with open(PID_FILE, "r") as f:
            pid = f.read().strip()
        print(f"Daemon is running (PID: {pid})")
    else:
        print("Daemon is not running.")


def run_job(force_colls=[]):
    """The main task to be run."""
    log("Starting stats update...")
    try:
        run_job_internal(force_colls)
    except Exception as e:
        log(f"ERROR stats updates failed. Error: {e}")

def run_job_internal(force_colls=[]):
  #Do stats update
  #Load config file every time, so we get change updates
  cfg=load_yaml(CFGFILE)
  stac_folder=os.path.join(cfg['config']['datastore_folder'],cfg['config']['stac_subfolder'])
  stats_folder=cfg['stats']['stats_folder']
  del cfg

  if len(force_colls)>0:
    collist=force_colls
  else:
    collist=[]
    with os.scandir(stac_folder) as it:
      for entry in it:
        if entry.is_dir(follow_symlinks=False):
          collist.append(entry.name)
    collist_file=os.path.join(stats_folder,'collections.list')
    with open(collist_file,'w') as f:
      json.dump(collist,f)
    log(f"Collection list written to {collist_file}")
  log(f"{len(collist)} collections to scan!")
  
  #Scan collection and write stats file
  for c in collist:
    stats_file=os.path.join(stats_folder,c+'.json')
    stats = check_collection(c)
    with open(stats_file, 'w') as f:
      json.dump(stats,f, ensure_ascii=False)
    log(f"Updated stats file {stats_file}")
  
  log("Stats retreival complete")


def verify_multihash(filepath,multihash):
  # Multihash format: <hash code><digest length><digest>
  # SHA2-256 code: 0x12, digest length: 32 bytes
  #Get function from hash code
  try:
    mh = bytes.fromhex(multihash)
  except Exception:
    return False
  algo_code = mh[0]
  digest = mh[2:]
  if algo_code == 0x11:
    h = hashlib.sha1()
  elif algo_code == 0x12:
    h = hashlib.sha256()
  elif algo_code == 0x13:
    h = hashlib.sha512()
  else:
    return False
  with open(filepath, 'rb') as f:
    for chunk in iter(lambda: f.read(8192), b''):
      h.update(chunk)
  return h.digest() == digest

def calculate_multihash(filepath):
  h = hashlib.sha256()
  with open(filepath, 'rb') as f:
    for chunk in iter(lambda: f.read(8192), b''):
      h.update(chunk)
  return '1220'+h.hexdigest()

#Returns
#0 format recommended and valid
#1 format recommended and invalid
#2 format cannot be checked
def verify_fileformat(filepath,fmt):
  fmt_beginning=fmt.split(';',1)[0]
  if fmt_beginning=='image/tiff':
    #We recommend geotiff or cogs. Check it is valid geotiff
    try:
      with open(filepath, 'rb') as f:
        # Read TIFF header
        header = f.read(8)
        if len(header) < 8:
          return 1
        # Determine endianness
        if header[:2] == b'II':
          endian = '<'  # little endian
        elif header[:2] == b'MM':
          endian = '>'  # big endian
        else:
          return 1
        # Check TIFF magic number (42)
        magic = struct.unpack(endian + 'H', header[2:4])[0]
        if magic != 42:
          return 1
        # Get offset to first IFD
        ifd_offset = struct.unpack(endian + 'I', header[4:8])[0]
        # Traverse IFDs
        while ifd_offset != 0:
          f.seek(ifd_offset)
          # Number of directory entries
          num_entries_bytes = f.read(2)
          if len(num_entries_bytes) < 2:
            return 1
          num_entries = struct.unpack(endian + 'H', num_entries_bytes)[0]
          # Read all directory entries
          entries_bytes = f.read(num_entries * 12)
          if len(entries_bytes) < num_entries * 12:
            return 1
          for i in range(num_entries):
            entry = entries_bytes[i*12:(i+1)*12]
            tag = struct.unpack(endian + 'H', entry[:2])[0]
            if tag == 34735:
              return 0  # Found GeoTIFF tag
          # Get offset to next IFD
          next_ifd_bytes = f.read(4)
          if len(next_ifd_bytes) < 4:
            return 1
          ifd_offset = struct.unpack(endian + 'I', next_ifd_bytes)[0]
        # If we traversed all IFDs and didn't find the tag
        return 1
    except Exception:
      return 1
  elif fmt_beginning in ['application/x-zarr','application/vnd+zarr','application/zip+zarr']:
    #For Zarr, we check if it is valid
    try:
      if os.path.isdir(filepath):
        zattrs_path = os.path.join(filepath, '.zattrs')
        if not os.path.isfile(zattrs_path): return 1
        with open(zarray_path, 'r') as f:
          data = json.load(f)
          del data
        zgroup_path = os.path.join(filepath, '.zgroup')
        if not os.path.isfile(zgroup_path): return 1
        with open(zgroup_path, 'r') as f:
          data = json.load(f)
          if 'zarr_format' not in data:
            return 1
        return 0
      else:
        with zipfile.ZipFile(filepath, 'r') as zf:
          with zf.open('.zgroup') as f:
            data = json.load(f)
            if 'zarr_format' not in data:
              return 1
            del data
          with zf.open('.zattrs') as f:
            data = json.load(f)
          return 0
    except Exception as e:
      print(e)
      return 1
  else:
    return 2

def get_directory_size(path):
    """
    Fast recursive directory size calculation using os.scandir().
    Returns size in bytes.
    """
    total_size = 0

    def scan_dir(p):
        nonlocal total_size
        try:
            with os.scandir(p) as it:
                for entry in it:
                    try:
                        if entry.is_file(follow_symlinks=False):
                            total_size += entry.stat(follow_symlinks=False).st_size
                        elif entry.is_dir(follow_symlinks=False):
                            scan_dir(entry.path)
                    except OSError:
                        # Skip files/directories we can't access
                        pass
        except OSError:
            # Skip directories we can't access
            pass

    scan_dir(path)
    return total_size

fix_file=None
unfix_file=None
diff_file=None
fix_file_num=0
def fix_product(stac_path,fixed_stac_item):
  #Write a batch file to apply fix and to revert it
  global fix_file, unfix_file, diff_file, fix_file_num
  fprefix=args.fix_script_prefix
  if fix_file is None:
    fix_file=open(f'{fprefix}.all.fix','w')
    unfix_file=open(f'{fprefix}.all.unfix','w')
    diff_file=open(f'{fprefix}.all.diff','w')
    fix_file.write("#!/bin/bash\n#This script applies patches to the metadata\n")
    unfix_file.write("#!/bin/bash\n#This script reverts the patches applied to the metadata\n")
    diff_file.write("#!/bin/bash\n#This script shows the patches to be applied\n")
  #Write the new stac file
  with open(f"{fprefix}.{fix_file_num}.fix",'w') as f:
    json.dump(fixed_stac_item,f)
  os.system(f"cp '{stac_path}' '{fprefix}.{fix_file_num}.unfix'")
  fix_file.write(f"cp -f '{fprefix}.{fix_file_num}.fix' '{stac_path}'\n")
  unfix_file.write(f"cp -f '{fprefix}.{fix_file_num}.unfix' '{stac_path}'\n")
  #Push new stac file to the catalogue
  cfg=load_yaml(CFGFILE)['config']
  cat_req_url=f"{cfg['catalogue_address']}/{fixed_stac_item['collection']}/items/{fixed_stac_item['id']}"
  fix_file.write(f"curl -k -s -S -H 'Content-Type: application/geo+json' -X PUT --data @{stac_path} '{cat_req_url}'\n")
  unfix_file.write(f"curl -k -s -S -H 'Content-Type: application/geo+json' -X PUT --data @{stac_path} '{cat_req_url}'\n")
  #Diff file
  diff_file.write(f"diff -U 0 --label '{stac_path}' --label '{fix_file_num}.fix' <(jq --sort-keys . '{stac_path}') <(jq --sort-keys . '{fprefix}.{fix_file_num}.fix')\n")
  #Increment file number
  fix_file_num+=1

#Error codes returned. Criticals are lower than 100
# 0   All valid (no error provided)
# 1   Invalid JSON
# 2   No assets of type data
# 100 Asset is a remote asset
# 3   Assets not found on disk
# 4   Asset size not found in JSON
# 5   Asset size mismatch on disk
# 101 Asset checksum not found in JSON
# 6   Asset checksum mismatch on disk
# 7   Asset format not found in JSON
# 8   Asset format mismatch on disk
# 102 Asset format is not recommended format (and cannot be validated)
def check_product(stac_path,product_assets_path):
  #Open the JSON file. If it is invalid, we have a problem
  try:
    with open(stac_path,'r') as f:
      stac_item = json.load(f)
  except Exception as e:
    return (0,0,0,{0:[1]})
  #Check if there are no assets
  if 'assets' not in stac_item:
    return (0,0,0,{0:[2]})
  #Check assets (record number of type data assets as there should be at least one
  num_data_assets=0
  num_assets=0
  size_assets=0
  errors={}
  product_needs_fix=False
  for asset_name in stac_item['assets']:
    num_assets=num_assets+1
    asset=stac_item['assets'][asset_name]
    #Check if the asset is of data type
    asset_is_data=False
    if 'roles' in asset and 'data' in asset['roles']:
      num_data_assets=num_data_assets+1
      asset_is_data=True
    #Check if asset is local or not
    if 'href' not in asset or asset['href'][0]!='/':
      errors.setdefault(asset_name, []).append(100)
      continue
    #Check if asset is on disk
    asset_file=os.path.join(product_assets_path,asset['href'].split('/')[-1])
    try:
      asset_stat=os.stat(asset_file,follow_symlinks=False)
    except Exception as e:
      errors.setdefault(asset_name, []).append(3)
      continue
    #Get asset size
    if stat.S_ISDIR(asset_stat.st_mode):
      asset_size=get_directory_size(asset_file)
    else:
      asset_size=asset_stat.st_size
    size_assets=size_assets+asset_size
    #Check asset size
    if 'file:size' not in asset:
      if 'fix_missing_size' in args and args.fix_missing_size:
        asset['file:size']=asset_size
        product_needs_fix=True
      else:
        errors.setdefault(asset_name, []).append(4)
    elif asset['file:size'] != asset_size:
      if 'fix_size_mismatch' in args and args.fix_size_mismatch:
        asset['file:size']=asset_size
        product_needs_fix=True
      else:
        errors.setdefault(asset_name, []).append(5)
        continue
    #Check asset checksum
    if 'file:checksum' not in asset:
      if 'fix_missing_checksum' in args and args.fix_missing_checksum:
        asset['file:checksum']=calculate_multihash(asset_file)
        product_needs_fix=True
      else:
        errors.setdefault(asset_name, []).append(101)
    elif not verify_multihash(asset_file,asset['file:checksum']):
      if 'fix_checksum_mismatch' in args and args.fix_checksum_mismatch:
        asset['file:checksum']=calculate_multihash(asset_file)
        product_needs_fix=True
      else:
        errors.setdefault(asset_name, []).append(6)
        continue
    #Check asset format type exists
    if 'type' not in asset:
      errors.setdefault(asset_name, []).append(7)
      continue
    #Check asset format is valid (only for data roles)
    if asset_is_data:
      asset_fmt_check=verify_fileformat(asset_file,asset['type'])
      if asset_fmt_check==1:
        errors.setdefault(asset_name, []).append(8)
        continue
      elif asset_fmt_check==2:
        errors.setdefault(asset_name, []).append(102)

  #There should be at least one data asset
  if num_data_assets==0:
    errors[0]=[2]

  #Generate product fix
  if product_needs_fix:
    fix_product(stac_path,stac_item)

  return (num_assets,num_data_assets,size_assets,errors)

#Checks a collection for assets correctness, will store any error found in the errorlog file, will return the total size of the 
def check_collection(coll_name):
  log(f"Scanning collection {coll_name}")
  #Load config file every time, so we get change updates
  cfg=load_yaml(CFGFILE)['config']
  asset_folder=os.path.join(cfg['datastore_folder'],cfg['assets_subfolder'])
  stac_folder=os.path.join(cfg['datastore_folder'],cfg['stac_subfolder'])
  stac_folder_len=len(stac_folder)
  del cfg
  #Get the stac folder path for the collection
  path=os.path.join(stac_folder,coll_name)
  #Total size and number of products to be returned
  totalSize=0
  numProducts=0
  numAssets=0
  numDataAssets=0
  errorProducts={}
  errorSummary=[0]*255
  #For the JSON folders by reg-api, you always have a path which is year/month/day
  with os.scandir(path) as it:
    #First level, year, no file should be here
    for entry in it:
      if entry.is_file(follow_symlinks=False):
        raise(Exception("Invalid STAC folder. There should be no file at {path}/{entry.name}"))
      elif entry.is_dir(follow_symlinks=False):
        #Second level, month, again no file should be here
        path2=os.path.join(path,entry.name)
        with os.scandir(path2) as it2:
          for entry2 in it2:
            if entry2.is_file(follow_symlinks=False):
              raise(Exception(f"ERROR: Invalid STAC folder. There should be no file at {path2}/{entry2.name}"))
            elif entry2.is_dir(follow_symlinks=False):
              #Third level, day, again no file should be here
              path3=os.path.join(path2,entry2.name)
              with os.scandir(path3) as it3:
                for entry3 in it3:
                  if entry3.is_file(follow_symlinks=False):
                    raise(Exception(f"ERROR: Invalid STAC folder. There should be no file at {path3}/{entry3.name}"))
                  elif entry3.is_dir(follow_symlinks=False):
                    #Forth level, product, only files here no directories
                    path4=os.path.join(path3,entry3.name)
                    path4_assets=asset_folder+path4[stac_folder_len:]
                    with os.scandir(path4) as it4:
                      for entry4 in it4:
                        if entry4.is_file(follow_symlinks=False):
                          #Check product assets, return number of assets and size of assets
                          path5=os.path.join(path4,entry4.name)
                          path5_assets=os.path.join(path4_assets,entry4.name)
                          product_check_results=check_product(path5,path5_assets)
                          numAssets=numAssets+product_check_results[0]
                          numDataAssets=numDataAssets+product_check_results[1]
                          totalSize=totalSize+product_check_results[2]
                          numProducts=numProducts+1
                          if len(product_check_results[3])>0:
                            errorProducts[entry4.name]=product_check_results[3]
                          for err_asset in product_check_results[3]:
                            err_codes = product_check_results[3][err_asset]
                            for ecode in err_codes:
                              errorSummary[ecode]+=1
                        elif entry4.is_dir(follow_symlinks=False):
                          raise(Exception(f"ERROR: Invalid STAC folder. There should be no directory at {path4}/{entry4.name}"))
  #return result
  return {"numProducts":numProducts,"numAssets":numAssets,"numDataAssets":numDataAssets,"totalSize":totalSize,"errorSummary":{k: v for k,v in enumerate(errorSummary) if v>0},"errorProducts":errorProducts}

#Main
parser = argparse.ArgumentParser(description="Offline checks on registered collections.")
subparsers = parser.add_subparsers()
parser_start = subparsers.add_parser('start', help='Start as deamon. Configuration is in the config file.')
parser_start.set_defaults(command='start')
parser_stop = subparsers.add_parser('stop', help='Stop the deamon.')
parser_stop.set_defaults(command='stop')
parser_status = subparsers.add_parser('status', help='Check deamon status.')
parser_status.set_defaults(command='status')
parser_run = subparsers.add_parser('run', help='Run in standalone mode. Good for testing, debug and fixing operations')
parser_run.set_defaults(command='run')
parser_run.add_argument('--fix-script-prefix', type=str, default='metadata', help='Prefix for the generated fix scripts. Defaults to the local folder')
parser_run.add_argument('--fix-missing-size', action='store_true', help='Fix missing file:size metadata, set it to current file size from disk. USE WITH CAUTION!')
parser_run.add_argument('--fix-size-mismatch', action='store_true', help='Fix file:size metadata mismatch on disk, override current and set it to file size from disk. USE WITH CAUTION!')
parser_run.add_argument('--fix-missing-checksum', action='store_true', help='Fix missing file:checksum metadata, set it to current file size from disk. USE WITH CAUTION!')
parser_run.add_argument('--fix-checksum-mismatch', action='store_true', help='Fix file:checksum metadata mismatch on disk, set it to current file checks from disk. USE WITH CAUTION!')
parser_run.add_argument('colls',nargs='*', help='Collections to perfom checks. If not set, all will be used')
args=parser.parse_args()

if args.command == "start":
  start()
elif args.command == "stop":
  stop()
elif args.command == "status":
  status()
elif args.command == "run":
  #Force full run (good for testing and next upgade)
  LOG_FILE='/dev/stdout'
  #Force single run
  run_job(args.colls)
  #Check if fixes are applied
  if fix_file is not None:
    fix_file.close()
    unfix_file.close()
    diff_file.close()
    print(f"There are products to fix.\nRun the 'bash {args.fix_script_prefix}.all.diff' bash script to check what fix will be applied.\nRun the 'bash -x {args.fix_script_prefix}.all.fix' to apply the fix.\nRun the 'bash -x {args.fix_script_prefix}.all.unfix' to revert to the original file.\nNOTE: Generated statistics refer to products after fixing is applied!")
else:
  print("Unknown command. Use start, stop, status or run.")
  sys.exit(1)
